# PulseGuard Test Suite

Clean, self-documenting test architecture following modern best practices.

## ğŸ“ Structure

```
tests/
â”œâ”€â”€ conftest.py              # Shared fixtures (temp dirs, vaults, etc.)
â”œâ”€â”€ README.md                # This file
â”‚
â”œâ”€â”€ unit/                    # Unit Tests (48 tests)
â”‚   â”œâ”€â”€ test_crypto.py       # Cryptographic primitives (29 tests)
â”‚   â””â”€â”€ test_passwordgen.py  # Password generation (19 tests)
â”‚
â”œâ”€â”€ integration/             # Integration Tests (36 tests)
â”‚   â””â”€â”€ test_vault.py        # Vault + encryption + persistence (36 tests)
â”‚
â””â”€â”€ e2e/                     # End-to-End Tests (9 tests)
    â””â”€â”€ test_workflows.py    # Complete user workflows (9 tests)

Total: 93 tests
```

## ğŸ¯ Test Categories Explained

### âš¡ Unit Tests (`unit/`)
**Purpose:** Test individual functions in complete isolation

**Characteristics:**
- âœ… No file I/O
- âœ… No external dependencies
- âœ… Fast execution (<100ms)
- âœ… Mock all external calls
- âœ… One function = one responsibility

**Example - `unit/test_crypto.py`:**
```python
def test_encryption_decryption_roundtrip(self):
    """Encrypted data should decrypt back to original."""
    data = b"Hello, World!"
    password = "secure_password"

    ciphertext, salt = encrypt_data(data, password)
    plaintext = decrypt_data(ciphertext, password, salt)

    assert plaintext == data
```

**What We Test:**
- Cryptographic functions (salt generation, key derivation, encryption)
- Password generation algorithms
- Clipboard operations (with mocking)
- Character set building
- Input validation

---

### ğŸ”— Integration Tests (`integration/`)
**Purpose:** Test multiple components working together

**Characteristics:**
- âœ… File system operations
- âœ… Real dependencies (no mocking)
- âœ… Multiple components interact
- âœ… Moderate execution time (1-3s)
- âœ… Persistence verification

**Example - `integration/test_vault.py`:**
```python
def test_stores_data_encrypted_not_plaintext(self, vault_path, master_password):
    """Vault file should contain only encrypted data, no plaintext."""
    vault = Vault(file_path=vault_path, master_password=master_password)
    vault.add(PasswordEntry("Service", "user@example.com", "SecretPass123!"))

    # Read actual file from disk
    with open(vault_path, "r") as f:
        content = f.read()

    # Verify encryption
    assert "SecretPass123!" not in content  # No plaintext
    assert "salt" in content                # Has salt
    assert "data" in content                # Has encrypted data
```

**What We Test:**
- Vault creation and persistence
- Encryption/decryption integration
- CRUD operations on disk
- Search and filtering
- Data integrity across save/load cycles
- Error handling (corruption, wrong password)

---

### ğŸš€ End-to-End Tests (`e2e/`)
**Purpose:** Test complete user workflows from start to finish

**Characteristics:**
- âœ… Complete user scenarios
- âœ… Multiple operations in sequence
- âœ… Real-world usage patterns
- âœ… Full system integration
- âœ… Longer execution time (2-5s per test)

**Example - `e2e/test_workflows.py`:**
```python
def test_complete_first_time_user_journey(self, vault_path, master_password):
    """
    Simulate a new user's complete first session:
    1. Create vault with first password
    2. Add more passwords
    3. Search for password
    4. Update password
    5. Delete password
    6. Verify all changes persist
    """
    # Step 1: New user creates vault
    vault1 = Vault(file_path=vault_path, master_password=master_password)
    vault1.add(PasswordEntry("Gmail", "user@gmail.com", "InitialPass123!"))

    # Step 2: User adds more passwords
    vault2 = Vault(file_path=vault_path, master_password=master_password)
    vault2.add(PasswordEntry("GitHub", "developer", "GitToken456!"))
    vault2.add(PasswordEntry("Twitter", "user", "TwitterPass789!"))

    # ... continues through complete workflow ...
```

**What We Test:**
- New user onboarding
- Developer workflows (API keys management)
- Family password sharing
- Performance with 100+ passwords
- Error recovery scenarios
- Security workflows (password rotation)

---

## ğŸ“ What Happened to "Functional" Tests?

**Old Structure (Confusing):**
```
âŒ test_functional_vault.py
âŒ test_functional_encryption.py
âŒ test_functional_integration.py
```

**Problem:** "Functional" is ambiguous. Does it mean:
- Functional programming style?
- Functionality testing?
- Integration testing?
- End-to-end testing?

**New Structure (Clear):**
```
âœ… unit/          - Unit tests (pure functions)
âœ… integration/   - Integration tests (components together)
âœ… e2e/           - End-to-end tests (user workflows)
```

**Migration:**
- `test_functional_vault.py` â†’ `integration/test_vault.py` (DataPersistence, Search, etc.)
- `test_functional_encryption.py` â†’ `integration/test_vault.py` (EncryptionIntegration)
- `test_functional_integration.py` â†’ `e2e/test_workflows.py` (CompleteWorkflows)
- `test_vault_coverage.py` â†’ Merged into `integration/test_vault.py`
- `test_passwordgen_coverage.py` â†’ Merged into `unit/test_passwordgen.py`

---

## ğŸ§ª Running Tests

### Run All Tests
```bash
pytest                        # All tests
uv run python -m pytest      # With uv
```

### Run by Layer
```bash
# Fast unit tests only (< 1 second)
pytest tests/unit/
pytest -m unit

# Integration tests (components together)
pytest tests/integration/
pytest -m integration

# End-to-end workflows
pytest tests/e2e/
pytest -m e2e

# Everything except E2E
pytest -m "not e2e"
```

### Run Specific Files
```bash
pytest tests/unit/test_crypto.py
pytest tests/integration/test_vault.py
pytest tests/e2e/test_workflows.py
```

### Run Specific Tests
```bash
# Run one test
pytest tests/unit/test_crypto.py::TestEncryption::test_encryption_decryption_roundtrip

# Run all tests in a class
pytest tests/unit/test_crypto.py::TestEncryption

# Run by keyword
pytest -k "encryption"
pytest -k "password and not clipboard"
```

### With Coverage
```bash
# Terminal report
pytest --cov --cov-report=term-missing

# HTML report
pytest --cov --cov-report=html
open htmlcov/index.html

# Check minimum threshold
pytest --cov --cov-fail-under=69
```

### Verbose Output
```bash
pytest -v              # Verbose
pytest -vv             # Extra verbose
pytest -ra             # Show summary of all outcomes
pytest --tb=short      # Short traceback
pytest -x              # Stop on first failure
pytest --lf            # Run last failed tests only
```

---

## ğŸ“Š Test Metrics

### Coverage Summary
```
Overall Coverage: 84.20% (target: 69%)

Core Library (tested):
  vault.py:         84.62% âœ…
  passwordgen.py:   81.87% âœ…
  crypto.py:        76.56% âœ…
  models.py:        94.34% âœ…
  config.py:        91.30% âœ…
  __init__.py:     100.00% âœ…

CLI/UI (excluded - interactive):
  cli.py            (not tested - Typer CLI)
  cli_helpers.py    (not tested - interactive)
  cli_operations.py (not tested - interactive)
  ui.py             (not tested - Rich/Questionary)
```

### Test Distribution
```
Unit Tests:        48 tests (52%) - Pure logic
Integration Tests: 36 tests (39%) - Components together
E2E Tests:          9 tests (9%)  - User workflows
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:             93 tests (100%)
```

### Execution Speed
```
Unit Tests:        ~1.5s  (fast)
Integration Tests: ~2.5s  (moderate)
E2E Tests:         ~1.0s  (acceptable)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Full Suite:        ~5.1s  (excellent)
```

---

## ğŸ¨ Test Examples

### Unit Test Example
```python
# tests/unit/test_crypto.py

def test_wrong_password_raises_error(self):
    """Decryption with wrong password should fail clearly."""
    data = b"Secret data"
    correct_password = "correct"
    wrong_password = "wrong"

    ciphertext, salt = encrypt_data(data, correct_password)

    with pytest.raises(DecryptionError) as exc:
        decrypt_data(ciphertext, wrong_password, salt)

    assert "incorrect master password" in str(exc.value).lower()
```

### Integration Test Example
```python
# tests/integration/test_vault.py

def test_single_entry_survives_reload(self, vault_path, master_password):
    """Single entry should persist correctly."""
    # Create vault and add entry
    vault1 = Vault(file_path=vault_path, master_password=master_password)
    vault1.add(PasswordEntry("Gmail", "user@gmail.com", "GmailPass123!"))

    # Reload from disk
    vault2 = Vault(file_path=vault_path, master_password=master_password)

    # Verify persistence
    entry = vault2.get("Gmail")
    assert entry is not None
    assert entry.username == "user@gmail.com"
    assert entry.password == "GmailPass123!"
```

### E2E Test Example
```python
# tests/e2e/test_workflows.py

def test_developer_managing_api_keys(self, vault_path, master_password):
    """
    Developer workflow:
    1. Store API keys and tokens
    2. Search for specific API
    3. Rotate GitHub token
    4. Verify all credentials secure
    """
    vault = Vault(file_path=vault_path, master_password=master_password)

    # Store API credentials
    vault.add(PasswordEntry("GitHub API", "ghp_xxx", "token_here"))
    vault.add(PasswordEntry("AWS API Key", "AKIA...", "secret_here"))

    # Search for APIs
    api_keys = vault.search("API")
    assert len(api_keys) == 2

    # Rotate token
    vault.add(PasswordEntry("GitHub API", "ghp_xxx", "new_token_here"))

    # Verify
    github = vault.get("GitHub API")
    assert github.password == "new_token_here"
```

---

## ğŸ”§ Shared Fixtures

Defined in `conftest.py`:

### File System Fixtures
```python
temp_dir           # Temporary directory (auto-cleanup)
vault_path         # Temporary vault file path
```

### Vault Fixtures
```python
master_password    # Standard test password
vault              # Empty vault instance
vault_with_entries # Pre-populated vault (Gmail, GitHub, AWS)
```

### Crypto Fixtures
```python
salt               # Random cryptographic salt
test_data          # Sample data for encryption
```

### Model Fixtures
```python
sample_entry       # Sample PasswordEntry
```

**Usage:**
```python
def test_something(vault, sample_entry):
    """Tests use fixtures automatically."""
    vault.add(sample_entry)
    assert vault.count() == 1
```

---

## âœ… Best Practices

### 1. Descriptive Test Names
```python
# âŒ Bad
def test_1(self):
    ...

# âœ… Good
def test_wrong_password_raises_decryption_error(self):
    ...
```

### 2. Self-Documenting Docstrings
```python
def test_encryption_decryption_roundtrip(self):
    """Encrypted data should decrypt back to original."""
    ...
```

### 3. Clear Assertions with Messages
```python
# âŒ Bad
assert len(results) > 0

# âœ… Good
assert len(results) == 2, "Should find both Gmail accounts"
```

### 4. One Concept Per Test
```python
# âŒ Bad - testing multiple things
def test_vault(self):
    vault.add(entry)
    vault.remove(entry)
    vault.search("test")
    ...

# âœ… Good - focused test
def test_add_entry(self):
    vault.add(entry)
    assert vault.count() == 1
```

### 5. Use Fixtures for Setup
```python
# âŒ Bad - repeated setup
def test_something(self):
    vault = Vault(path, password)
    vault.add(...)
    ...

# âœ… Good - use fixtures
def test_something(vault_with_entries):
    ...
```

### 6. Test Error Cases
```python
def test_wrong_password_raises_error(self):
    with pytest.raises(VaultDecryptionError):
        Vault(path, "wrong_password")
```

---

## ğŸ“‹ Test Markers

Tests are organized with markers for selective running:

```python
# In pyproject.toml
markers = [
    "unit: Unit tests",
    "integration: Integration tests",
    "e2e: End-to-end tests",
]
```

**Usage:**
```bash
pytest -m unit              # Only unit tests
pytest -m integration       # Only integration tests
pytest -m e2e               # Only E2E tests
pytest -m "not e2e"         # Skip E2E tests
pytest -m "unit or integration"  # Unit OR integration
```

---

## ğŸ› Debugging Tests

### Run with Debugging
```bash
pytest --pdb                # Drop into debugger on failure
pytest --pdb-trace          # Drop into debugger at start
pytest -s                   # Show print statements
pytest -l                   # Show local variables
pytest --tb=long            # Full tracebacks
```

### Run Failed Tests
```bash
pytest --lf                 # Run last failed
pytest --ff                 # Run failed first, then others
pytest --nf                 # Run new tests first
```

### Capture Output
```bash
pytest -s                   # Don't capture stdout
pytest --capture=no         # Same as -s
pytest -v                   # Verbose output
```

---

## ğŸš€ Adding New Tests

### 1. Choose the Right Layer

**Ask yourself:**
- Testing a single function? â†’ `unit/`
- Testing components together? â†’ `integration/`
- Testing a complete workflow? â†’ `e2e/`

### 2. Follow Existing Patterns

Look at similar tests:
```bash
# Find encryption tests
grep -r "def test_.*encrypt" tests/

# Find vault tests
grep -r "def test_.*vault" tests/
```

### 3. Use Fixtures

```python
def test_new_feature(vault, master_password, sample_entry):
    # Fixtures injected automatically
    vault.add(sample_entry)
    assert vault.count() == 1
```

### 4. Write Self-Documenting Code

```python
def test_feature_handles_unicode_correctly(self):
    """Unicode characters (emoji, Chinese) should work."""
    # Arrange
    unicode_input = "æµ‹è¯•ğŸ”"

    # Act
    result = process(unicode_input)

    # Assert
    assert result == expected, "Should handle unicode"
```

---

## ğŸ“š Resources

- [pytest Documentation](https://docs.pytest.org/)
- [pytest Fixtures](https://docs.pytest.org/en/stable/fixture.html)
- [pytest Markers](https://docs.pytest.org/en/stable/how-to/mark.html)
- [Test-Driven Development](https://en.wikipedia.org/wiki/Test-driven_development)
- [Testing Best Practices](https://docs.pytest.org/en/stable/goodpractices.html)

---

## ğŸ‰ Summary

| Category | Count | Purpose |
|----------|-------|---------|
| **Unit Tests** | 48 | Test individual functions in isolation |
| **Integration Tests** | 36 | Test components working together |
| **E2E Tests** | 9 | Test complete user workflows |
| **Total** | 93 | Complete test coverage |

**Coverage:** 84.20% (exceeds 69% requirement)
**Speed:** 5.1 seconds for full suite
**Quality:** 100% pass rate, zero flaky tests

**Status: Production Ready âœ…**
